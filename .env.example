# LLMFlow Environment Variables
# Copy this file to .env and fill in your values

# ===========================================
# Core Settings
# ===========================================

# Proxy server port (receives LLM API requests)
PROXY_PORT=8080

# Dashboard and OTLP ingestion port
DASHBOARD_PORT=3000

# Data directory for SQLite database
DATA_DIR=~/.llmflow

# Maximum traces to retain (older traces are pruned)
MAX_TRACES=10000

# Enable verbose logging (0=off, 1=on)
VERBOSE=0

# ===========================================
# Provider API Keys
# ===========================================

# OpenAI (default provider)
OPENAI_API_KEY=sk-...

# Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-...

# Google Gemini (either works)
GOOGLE_API_KEY=
GEMINI_API_KEY=

# Cohere
COHERE_API_KEY=

# Groq
GROQ_API_KEY=gsk_...

# Mistral AI
MISTRAL_API_KEY=

# Together AI
TOGETHER_API_KEY=

# Perplexity
PERPLEXITY_API_KEY=pplx-...

# OpenRouter
OPENROUTER_API_KEY=sk-or-...

# ===========================================
# Provider-Specific Settings
# ===========================================

# Ollama (local LLM server)
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# Azure OpenAI
AZURE_OPENAI_RESOURCE=your-resource-name
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_VERSION=2024-02-01
# Optional: map models to deployments
# AZURE_DEPLOYMENT_GPT_4=gpt-4-deployment
# AZURE_DEPLOYMENT_GPT_35_TURBO=gpt-35-turbo-deployment
